{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Decor Sales Forecasting - ChromeAccessories"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Summary of the Code below :\n",
    "1. Establish MySQL Connection and load data\n",
    "2. Data Preprocessing (Typecasting and Resampling daily data to monthly)\n",
    "3. Visualizing Rolling statistics to observe variation in mean and standard deviation for selected Feature.\n",
    "4. Checking for Data Stationarity using Augmented Dickey-Fuller Test for the feature\n",
    "5. Hyper-parameter Tuning using ACF and PACF plots for building SARIMA Model (this process takes little time)\n",
    "6. Models\n",
    "   (a) SARIMA\n",
    "   (b) HoltWinters Exponential Smoothing with Additive Seasonality & Additive Trend\n",
    "   (c) FB Prophet\n",
    "   (d) Auto Time Series\n",
    "7. Evaluation of the Models\n",
    "8. Saving the model with least MAPE\n",
    "9. Loading saved model (.pkl) to predict sales for 12 months.\n",
    "10. Closing MySQL Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Connecting Python to MySQL for fetching data \n",
    "import mysql.connector\n",
    "\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### MySQL Connection to fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    connection = mysql.connector.connect(host='localhost',\n",
    "                                        database='car_decors',\n",
    "                                         user='root',\n",
    "                                         password='***********')\n",
    "\n",
    "    sql_select_Query = \"SELECT * FROM decorsales\"\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(sql_select_Query)\n",
    "    columns = len(cursor.description)\n",
    "    columns = [i[0] for i in cursor.description]\n",
    "    print(columns)\n",
    "\n",
    "    # get all records\n",
    "    records = cursor.fetchall()\n",
    "    print(\"Total number of rows in table: \", cursor.rowcount)\n",
    "\n",
    "except mysql.connector.Error as e:\n",
    "    print(\"Error reading data from MySQL table\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Converting fetched records to Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = np.array(records)\n",
    "records = records[:,0:25]\n",
    "decor_sales=pd.DataFrame(records,columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Type Casting Date and other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decor_sales.dtypes\n",
    "decor_sales.Date = pd.to_datetime(decor_sales.Date)\n",
    "decor_sales.iloc[:,1:] = decor_sales.iloc[:,1:].astype(\"int32\")\n",
    "decor_sales.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creating Subset of Decor Sales Dataset and resampling Monthly Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = decor_sales\n",
    "df = df.set_index('Date')\n",
    "df = df.resample(\"MS\").sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note : Time period options when resampling a time series # MS - Monthly ; W - Weekly ; QS - Quarterly ; YS - Yearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"figure\", figsize=(16,8))\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Rolling statistics to observe variation in mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = df ['ChromeAccessories']\n",
    "timeseries.rolling(12).mean().plot(label='12 Month Rolling Mean', marker='.')\n",
    "timeseries.rolling(12).std().plot(label='12 Month Rolling Std', marker='.')\n",
    "timeseries.plot(marker='.')\n",
    "plt.title('Rolling Statistics to observe variation in Mean and Standard Deviation', fontsize = 18, fontweight = 'bold')\n",
    "plt.xlabel('Year', fontsize = 14)\n",
    "plt.ylabel('Sales (Number of Units)', fontsize = 14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The plot shows, there is nearly a constant mean and standard deviation except noise in Qtr 2 - 2020 (Lockdown period)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Checking Seasonalty and Trend components for the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "add = seasonal_decompose(df[\"ChromeAccessories\"],model=\"additive\",period=12)\n",
    "add.plot();"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Decomposition plot shows constant trend with noise in Qtr 2 - 2020 and seasonality is additive in nature.\n",
    "# The data is seasonal and follows constant trend.\n",
    "# Also, the average value or the mean of the residuals seem to be zero which holds our assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking for Data Stationarity using Augmented Dickey-Fuller Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def check_adf(time_series):\n",
    "    test_result = adfuller(df['ChromeAccessories'])\n",
    "    print ('ADF Test:')\n",
    "    labels = ['ADF Statistic','p-value','No. of Lags Used','Number of Observations Used']\n",
    "\n",
    "    for value,label in zip(test_result,labels):\n",
    "        print (label+': '+str(value)+str(\"\\n\"))\n",
    "        if test_result [1] <= 0.05:\n",
    "            print (\"Reject null hypothesis; Data is stationary\")\n",
    "        else:\n",
    "            print (\"Fail to reject H0; Data is non-stationary\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If the data is non-stationary so we need to apply differencing to make our data stationary.\n",
    "df ['ChromeAccessories'] = df ['ChromeAccessories'] - df ['ChromeAccessories']. shift (1)\n",
    "adf_check(df['ChromeAccessories'].dropna())\n",
    "If again data is non-stationary we need to differencing with subsequent shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_adf(df['ChromeAccessories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adfuller test Results for all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "def adfuller_parameter(x):\n",
    "    P = []\n",
    "    columns = []\n",
    "    used_lag = []\n",
    "    for i in x.columns:\n",
    "        test_stats,p,used_lags,nobs,critical_value,ic_best = adfuller(x[i])\n",
    "        columns.append(i)\n",
    "        P.append(p)\n",
    "        used_lag.append(used_lags)\n",
    "    return pd.DataFrame({\"COLUMNS\":columns,\"P_VALUE\":P,\"MAX_USED_LAG\":used_lag})\n",
    "\n",
    "adfuller_parameter(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "By looking at adfuller test result we conclude that we need differencing by 0 shifts to make our data stationary for android headunits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyper-parameter Tuning # Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# By looking at ACF pot and PACF plot we decide the value p(Auto regressive) and q(Moving average)\n",
    "# p = sudden shuts off in pacf plot.\n",
    "# q = Exponential drop in acf plot.\n",
    "# d = degree of differencing/shift by adfuller test\n",
    "\n",
    "#Auto Regressive (p)\n",
    "# Identification of an AR model is often best done with the PACF.\n",
    "# For an AR model, the theoretical PACF “shuts off” past the order of the model. \n",
    "# The phrase “shuts off” means that in theory the partial autocorrelations are equal to 0 beyond that point. \n",
    "# Put another way, the number of non-zero partial autocorrelations gives the order of the AR model.\n",
    "# By the “order of the model” we mean the most extreme lag of x that is used as a predictor.\n",
    "\n",
    "# Integration (d)\n",
    "# Integration paramter is choosen through  how much value you have differentiated from original\n",
    "# For a stationary data its either be 0 or 1\n",
    "\n",
    "# Moving Average (q) \n",
    "# the theoretical PACF does not shut off, but instead tapers or exponetially decrease toward 0 in some manner.\n",
    "# A clearer pattern for an MA model is in the ACF.\n",
    "# The ACF will have non-zero autocorrelations only at lags involved in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
    "sm.graphics.tsa.plot_acf(df[\"ChromeAccessories\"], lags=12, title = 'ACF Plot', ax=ax[0])\n",
    "sm.graphics.tsa.plot_pacf(df[\"ChromeAccessories\"], lags=12, title = 'PACF Plot',ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building - SARIMA Model ( Seasonal ARIMA Model )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[\"ChromeAccessories\"].iloc[0:int(len(df)*.95)] #train model with approx 95% data\n",
    "test_df = df[\"ChromeAccessories\"].iloc[int(len(train_df)):] #test model with 5% data\n",
    "\n",
    "print(\"Train_df : \",len(train_df))\n",
    "print(\"Test_df : \",len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### User Defined Function to calculate the MAPE value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Automated Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import itertools as i \n",
    "p = range(0,3) \n",
    "d = range(0,2)\n",
    "q = range(0,3)\n",
    "\n",
    "pdq_combo = list(i.product(p,d,q)) #this will all combination of p,d,q throgh a tuple \n",
    "\n",
    "error = []\n",
    "aic_sarima = []\n",
    "order_arima = []\n",
    "order_sarima = []\n",
    "seasonality = 12\n",
    "for pdq in pdq_combo:\n",
    "    for PDQ in pdq_combo:\n",
    "        try:\n",
    "            SEASONAL_ORDER = list(PDQ)\n",
    "            SEASONAL_ORDER.append(seasonality)\n",
    "            model = sm.tsa.SARIMAX(train_df,order=(pdq),seasonal_order=tuple(SEASONAL_ORDER))\n",
    "            result = model.fit(disp=0)\n",
    "            pred = result.predict(start=len(train_df),end=len(df)-1)\n",
    "            eror = mape(test_df,pred)\n",
    "            aic_sarima.append(result.aic)\n",
    "            order_arima.append(pdq)\n",
    "            order_sarima.append(tuple(SEASONAL_ORDER))\n",
    "            error.append(eror)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe of seasonality orders and errors \n",
    "df_error = pd.DataFrame({\"arima_order\":order_arima,\"sarima_order\": order_sarima,\"error\":error,\"aic\":aic_sarima})\n",
    "df_error = df_error.sort_values(by=\"error\",ascending = True)\n",
    "df_error.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best parameter selection\n",
    "p_d_q = df_error.iloc[0,0] #choosing best parameter for arima order\n",
    "P_D_Q = df_error.iloc[0,1] #choosing best parameter for seasonal  order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best parameter selection\n",
    "print(\"Best p_d_q parameter : \", p_d_q)\n",
    "print(\"Best P_D_Q parameter : \", P_D_Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model with best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_model = sm.tsa.SARIMAX(train_df, order=(p_d_q), seasonal_order=(P_D_Q))\n",
    "sarima_results = sarima_model.fit(disp = 0)\n",
    "sarima_pred = sarima_results.predict(start=test_df.index[0],end=test_df.index[-1])\n",
    "sarima_pred_large = sarima_results.predict(start=75,end=86,dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sarima_results.summary())\n",
    "sarima_diagnostics = sarima_results.plot_diagnostics(figsize=(16,8))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Insights from these diagnostic plot :\n",
    "# 1.The top left plot shows the residuals over time.\n",
    "# The plot shows our residuals are fluctuating around mean 0 there is uniform deviation over time\n",
    "# except some noise in second quarter of 2021 due to lockdown imposed by government with effect of COVID-19 pandemic.\n",
    "\n",
    "# 2.In the top-right plot,\n",
    "# We see that the KDE follows closely with the N(0,1) line to indicate that the residuals are normally distributed. \n",
    "# This line is the standard notation for a normal distribution with a mean of 0 and a standard deviation of 1.\n",
    "# In our plot residuals are normally distributed.\n",
    "\n",
    "# 3.In the bottom left qq-plot,\n",
    "# We see the ordered distribution of residuals(blue dots) following the linear trend(red line)\n",
    "# of the samples taken from a standard normal distribution with N(0, 1).\n",
    "\n",
    "# 4.The autocorrelation visual (called a “correlogram”) on the bottom right shows that-\n",
    "# The time series residuals have a low correlation with the lagged versions of itself \n",
    "# (that is, the majority of dots fall into the blue shaded area)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted values\n",
    "# Point estimation\n",
    "sarima_prediction = sarima_results.get_prediction(start = test_df.index[0], end = test_df.index[-1], dynamic = True, full_results = True)\n",
    "sarima_point_estimation = sarima_prediction.predicted_mean\n",
    "sarima_point_estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking MAPE\n",
    "mape(test_df, sarima_point_estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At 95% confidence interval\n",
    "sarima_pred_range = sarima_prediction.conf_int(alpha = 0.05)\n",
    "sarima_pred_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting Sarima Prediction\n",
    "plt.plot(train_df,color=\"g\",label=\"Train Data\", marker='.')\n",
    "plt.plot(test_df,color=\"b\",label=\"Test Data\", marker='.')\n",
    "plt.plot(sarima_point_estimation,color=\"r\",label=\"Forecast (Test Data)\", marker='.')\n",
    "plt.figtext(0.13, 0.15, '\\nMAPE     :  {} \\nSARIMA :  {},{} \\nAIC        :  {}'.format(mape(test_df, sarima_point_estimation), p_d_q, P_D_Q, sarima_results.aic, fontsize = 11))\n",
    "plt.fill_between(sarima_pred_range.index,sarima_pred_range.iloc[:,0],sarima_pred_range.iloc[:,1],color='b',alpha=.2)\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "############################################################################################################################ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holt Winters Exponential Smoothing with Additive Seasonality and Additive Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing # \n",
    "\n",
    "hwe_model_add_add = ExponentialSmoothing(train_df, seasonal =\"add\", trend = \"add\", seasonal_periods = 12).fit()\n",
    "pred_hwe_add_add = hwe_model_add_add.predict(start = test_df.index[0], end = test_df.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_hwe_add_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plotting Holt Winters Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_df,color=\"g\",label=\"Train Data\")\n",
    "plt.plot(test_df,color=\"b\",label=\"Test Data\")\n",
    "plt.plot(pred_hwe_add_add,color=\"r\",label=\"Forecast (Test Data)\")\n",
    "plt.suptitle('Model : Holt Winters', fontsize = 12, fontweight = 'bold')\n",
    "plt.title('Car Decors - ANDROID HEAD UNITS', fontsize = 18, fontweight = 'bold')\n",
    "plt.figtext(0.13, 0.14, '\\nMAPE     :  {} \\nAIC        :  {}'.format(mape(test_df, pred_hwe_add_add), hwe_model_add_add.aic))\n",
    "plt.xlabel('Year', fontsize = 14)\n",
    "plt.ylabel('Sales (Number of Units)', fontsize = 14)\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(test_df, pred_hwe_add_add) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "############################################################################################################################ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FB Prophet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Libraries\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.plot import plot_plotly\n",
    "\n",
    "df1 = decor_sales\n",
    "df1 = df1.set_index('Date')\n",
    "df1 = df1.resample(\"MS\").sum()\n",
    "df1.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1 = df1[[\"Date\",\"ChromeAccessories\"]].iloc[0:int(len(df1)*.95)] #train model with approx 95% data\n",
    "test_df1 = df1[[\"Date\",\"ChromeAccessories\"]].iloc[int(len(train_df1)):] #test model with 5% data\n",
    "\n",
    "print(\"Train : \",len(train_df1))\n",
    "print(\"Test : \",len(test_df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1.columns = [\"ds\",\"y\"]\n",
    "test_df1.columns = [\"ds\",\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the Model\n",
    "prophet_model = Prophet().fit(train_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the period for which we want a prediction\n",
    "future = list()\n",
    "for i in range(1, 5):\n",
    "\tdate = '2021-%02d' % i\n",
    "\tfuture.append([date])\n",
    "future = pd.DataFrame(future)\n",
    "future.columns = ['ds']\n",
    "future['ds']= pd.to_datetime(future['ds'])\n",
    "future "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = prophet_model.predict(future)\n",
    "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1=test_df1.set_index(\"ds\")\n",
    "train_df1 = train_df1.set_index(\"ds\")\n",
    "forecast=forecast.set_index(\"ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.plot(train_df1['y'],color=\"r\",label=\"Train Data\")\n",
    "plt.plot(test_df1['y'],color=\"b\",label=\"Test Data\")\n",
    "plt.plot(forecast[\"yhat\"],color=\"g\",label=\"Forecast (Test Data)\")\n",
    "plt.grid( linestyle='-', linewidth=2)\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPE\n",
    "mape(test_df1['y'], forecast['yhat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "sqrt(mean_squared_error(test_df1['y'], forecast['yhat'].tail(4)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "############################################################################################################################ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Time Series Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_ts import auto_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = train_df1\n",
    "test_df2 = test_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_model = auto_timeseries( score_type='rmse', time_interval='MS', non_seasonal_pdq=(12,12,12), seasonality=True, seasonal_period=12, model_type=\"best\", verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_model.fit(traindata= train_df2, ts_column=\"ds\", target=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_model.get_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_model.plot_cv_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_predictions = ts_model.predict(test_df2, model='best')\n",
    "future_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the period for which we want a prediction\n",
    "ts_future = list()\n",
    "for i in range(1, 5):\n",
    "\tdate = '2021-%02d' % i\n",
    "\tts_future.append([date])\n",
    "ts_future = pd.DataFrame(ts_future)\n",
    "ts_future.columns = ['ds']\n",
    "ts_future['ds']= pd.to_datetime(ts_future['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_model.predict(ts_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(test_df2[\"y\"],future_predictions[\"yhat\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "############################################################################################################################ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "print(\"\\nSARIMA Trend          :  \", p_d_q)\n",
    "print(\"SARIMA Seasonal Order :  \", P_D_Q)\n",
    "print(\"SARIMA AIC            :  \", sarima_results.aic)\n",
    "print(\"SARIMA RMSE           :  \", np.sqrt(mse(test_df,sarima_point_estimation)))\n",
    "print(\"SARIMA MAPE           :  \", mape(test_df, sarima_point_estimation))\n",
    "print(\"\\nHolt Winters AIC      :  \", hwe_model_add_add.aic)\n",
    "print(\"Holt Winters RMSE     :  \", np.sqrt(mse(test_df,pred_hwe_add_add)))\n",
    "print(\"Holt Winters MAPE     :  \", mape(test_df, pred_hwe_add_add))\n",
    "print(\"\\nFB Prophet RMSE       :  \", sqrt(mean_squared_error(test_df1['y'], forecast['yhat'])))\n",
    "print(\"FB Prophet MAPE       :  \", mape(test_df1['y'], forecast['yhat']))\n",
    "print(\"\\nAuto Time Series: \\n  \", ts_model.get_leaderboard())\n",
    "print(\"Auto Time Series MAPE       :  \", mape(test_df2[\"y\"],future_predictions[\"yhat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima = mape(test_df, sarima_point_estimation)\n",
    "hwinters = mape(test_df, pred_hwe_add_add)\n",
    "fbprophet = mape(test_df1['y'], forecast['yhat'])\n",
    "autots = mape(test_df2[\"y\"],future_predictions[\"yhat\"])\n",
    "\n",
    "mape_data = {'models':['SARIMA','HOLTWINTERS','FB_PROPHET','AUTO_TS'], 'name':['sarima_model', 'hwe_model_add_add','prophet_model','ts_model'],'mape':[sarima, hwinters, fbprophet, autots]}\n",
    "mape_error = pd.DataFrame(mape_data)\n",
    "mape_error = mape_error.sort_values(by=\"mape\",ascending = True)\n",
    "mape_error.reset_index(inplace=True,drop=True)\n",
    "#best_model = mape_error.iloc[0,0]\n",
    "print('\\033[1m'+\"Best Model with lowest MAPE : \", mape_error.iloc[0,0] + \" ( \" + mape_error.iloc[0,1] + \" ) \" + '\\033[0m')\n",
    "print(\"\\nMAPE ERRORS :\\n\\n\", mape_error)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "############################################################################################################################ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'sarima_ca.pkl'\n",
    "pickle.dump(sarima_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Testing saved Model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Model summary and diagnstics plot #######\n",
    "with open(filename, \"rb\") as file:\n",
    "    load_model = pickle.load(file)\n",
    "    \n",
    "result = load_model.fit()\n",
    "#print(result.summary())\n",
    "#diagnostics = result.plot_diagnostics(figsize=(16,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = result.get_prediction(start = 76, end = 87, dynamic = False)\n",
    "\n",
    "# Point estimation\n",
    "prediction = pred.predicted_mean\n",
    "prediction = round(prediction)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting final Sarima Prediction\n",
    "plt.plot(df['ChromeAccessories'],color=\"g\",label=\"Actual\", marker='.')\n",
    "plt.plot(prediction,color=\"r\",label=\"Forecast\", marker='.')\n",
    "plt.suptitle('Model : SARIMA', fontsize = 12, fontweight = 'bold')\n",
    "plt.title('Car Decors - Chrome Accessories', fontsize = 18, fontweight = 'bold')\n",
    "plt.figtext(0.13, 0.14, '\\nMAPE     :  {} \\nAIC        :  {}'.format(mape(test_df, sarima_point_estimation), sarima_results.aic))\n",
    "plt.xlabel('Year', fontsize = 14)\n",
    "plt.ylabel('Sales (Number of Units)', fontsize = 14)\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing connection to MySQL and clearing variables from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if connection.is_connected():\n",
    "#    connection.close()\n",
    "#    cursor.close()\n",
    "#    print(\"MySQL connection is closed\")\n",
    "\n",
    "# Clear all variables from memory\n",
    "#globals().clear()\n",
    "\n",
    "#####################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
